{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree\n",
        "\n",
        "1. Initialization: Start with the root node of the tree.\n",
        "\n",
        "2. Splitting: At each node, determine the best split based on a chosen criterion (e.g., Gini impurity, information gain). This involves evaluating all possible splits for each feature and selecting the one that maximizes the criterion.\n",
        "\n",
        "3. Stopping Criteria: Check stopping criteria to determine whether to stop growing the tree. Stopping criteria may include reaching the maximum depth, having fewer than a certain number of samples, or achieving perfect purity (homogeneity) in a leaf node.\n",
        "\n",
        "4. Recursive Splitting: If the stopping criteria are not met, recursively split the data into subsets based on the chosen split criterion. Each subset becomes a child node of the current node.\n",
        "\n",
        "5. Repeat: Repeat steps 2-4 for each child node until the stopping criteria are met for all nodes, or until no further improvement can be made.\n",
        "\n",
        "6. Leaf Node Assignment: Once the tree has been built, assign a class label to each leaf node based on the majority class of the samples in that node.\n",
        "\n",
        "7. Pruning (Optional): Prune the tree to prevent overfitting. Pruning techniques may involve removing nodes that do not significantly improve the performance of the tree on a validation set or applying post-pruning techniques to simplify the tree structure.\n",
        "\n",
        "8. Final Decision Tree: The resulting tree represents the decision boundaries learned from the training data, with each internal node representing a decision based on a feature and each leaf node representing a class label.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6w4L8btsMdVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a more detailed breakdown of the steps involved in splitting a node (Step 2):\n",
        "\n",
        "* Calculate impurity: Calculate the impurity measure for the current node based\n",
        "\n",
        "\n",
        "* on the chosen criterion (e.g., Gini impurity, information gain). Impurity measures how mixed the classes are within the node.\n",
        "\n",
        "* Evaluate potential splits: For each feature, evaluate potential split points to determine the best split that minimizes impurity. This involves sorting the feature values and evaluating the impurity at each split point.\n",
        "\n",
        "* Select best split: Choose the feature and split point that result in the lowest impurity (or highest information gain) among all features and split points evaluated.\n",
        "\n",
        "* Split the node: Split the node into two child nodes based on the chosen feature and split point. Samples with feature values below the split point go to the left child node, while samples with feature values above the split point go to the right child node.\n",
        "\n",
        "* Repeat for child nodes: Recursively repeat the splitting process for each child node until the stopping criteria are met or no further improvement can be made.\n",
        "\n",
        "This process continues recursively until all stopping criteria are met, resulting in the construction of the decision tree.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vBWTSlieNNhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt0JjMBaL_PQ"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature_index = feature_index  # Index of the feature to split on\n",
        "        self.threshold = threshold          # Threshold value for the feature\n",
        "        self.left = left                    # Left child node\n",
        "        self.right = right                  # Right child node\n",
        "        self.value = value                  # Value (class label) if the node is a leaf\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth        # Maximum depth of the tree\n",
        "        self.root = None                  # Root node of the decision tree\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(set(y))\n",
        "\n",
        "        # Stopping criteria\n",
        "        if depth == self.max_depth or n_classes == 1 or n_samples < 2:\n",
        "            return Node(value=self._most_common_label(y))\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold = self._find_best_split(X, y)\n",
        "\n",
        "        # Split the data\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        X_left, y_left = X[left_indices], y[left_indices]\n",
        "        X_right, y_right = X[~left_indices], y[~left_indices]\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left = self._build_tree(X_left, y_left, depth + 1)\n",
        "        right = self._build_tree(X_right, y_right, depth + 1)\n",
        "\n",
        "        return Node(feature_index=best_feature, threshold=best_threshold, left=left, right=right)\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        best_gini = float('inf')\n",
        "        best_feature, best_threshold = None, None\n",
        "\n",
        "        for feature_index in range(X.shape[1]):\n",
        "            thresholds = sorted(set(X[:, feature_index]))\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature_index] < threshold\n",
        "                y_left = y[left_indices]\n",
        "                y_right = y[~left_indices]\n",
        "\n",
        "                gini = self._gini_impurity(y_left) + self._gini_impurity(y_right)\n",
        "\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_feature = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _gini_impurity(self, y):\n",
        "        classes = set(y)\n",
        "        impurity = 1.0\n",
        "\n",
        "        for cls in classes:\n",
        "            p_cls = sum(y == cls) / len(y)\n",
        "            impurity -= p_cls ** 2\n",
        "\n",
        "        return impurity\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        return max(set(y), key=y.tolist().count)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict_sample(x, self.root) for x in X]\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        elif x[node.feature_index] < node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        else:\n",
        "            return self._predict_sample(x, node.right)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the decision tree model from scikit-learn\n",
        "sklearn_tree = DecisionTreeClassifier(random_state=42)\n",
        "sklearn_tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "sklearn_predictions = sklearn_tree.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of scikit-learn decision tree model\n",
        "sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)\n",
        "\n",
        "# Fit the decision tree model implemented from scratch\n",
        "scratch_tree = DecisionTreeClassifier(max_depth=3)\n",
        "scratch_tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set using the scratch implementation\n",
        "scratch_predictions = scratch_tree.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of scratch decision tree model\n",
        "scratch_accuracy = accuracy_score(y_test, scratch_predictions)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Accuracy of scikit-learn Decision Tree:\", sklearn_accuracy)\n",
        "print(\"Accuracy of Scratch Decision Tree:\", scratch_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hhE5LSqMKnE",
        "outputId": "18a04f41-39d8-401b-fa40-b0cd727a26a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of scikit-learn Decision Tree: 1.0\n",
            "Accuracy of Scratch Decision Tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lFoj4nPNMa7F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPtSR1oMMLsv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}